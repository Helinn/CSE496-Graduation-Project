{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"this python file is used to construct the fake data for the model\"\"\"\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "import Tweet as tweet\n",
    "from datetime import datetime\n",
    "\n",
    "from networkx import *\n",
    "import networkx.algorithms as nxalg\n",
    "\n",
    "def read_data_from_database():\n",
    "    mydb = mysql.connector.connect(\n",
    "        user = \"root\",\n",
    "        password = \"D12345\",\n",
    "        host = \"localhost\",\n",
    "        database = \"tweetermysql_2018-07-30\"\n",
    "    )\n",
    "\n",
    "    my_cursor = mydb.cursor()\n",
    "\n",
    "    my_cursor.execute(' select ID, MetaData, Date, Section from news_2017_01 limit 20')\n",
    "\n",
    "    my_result = my_cursor.fetchall()\n",
    "    \n",
    "    data = list()\n",
    "    \n",
    "    for item in my_result:\n",
    "        u, m_data, date, section = item\n",
    "        t = tweet.Tweet(u,section,m_data,datetime.strptime(date,'%Y-%m-%d'))\n",
    "        data.append(t)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_word_id_dictionary(tweets):\n",
    "    word_id_dict = {}\n",
    "    id = 0\n",
    "    for tweet in tweets:\n",
    "        if not word_id_dict.get(tweet.user):\n",
    "            word_id_dict[tweet.user] = id\n",
    "            id += 1 \n",
    "        if not word_id_dict.get(datetime.strftime(tweet.date,'%Y-%m-%d')):\n",
    "            word_id_dict[datetime.strftime(tweet.date,'%Y-%m-%d')] = id\n",
    "            id += 1 \n",
    "        for word in tweet.text.split():\n",
    "            if not word_id_dict.get(word):\n",
    "                word_id_dict[word] = id\n",
    "                id += 1\n",
    "        #print(tweet.user,tweet.text,datetime.strftime(tweet.date,'%Y-%m-%d'))\n",
    "    \n",
    "    return word_id_dict\n",
    "def create_graph(tweet):\n",
    "    G1 = nx.DiGraph()\n",
    "    #for tweet in tweets:\n",
    "    G1.add_node(word_id_dict[tweet.user])\n",
    "    tweetFormat = str(tweet.Id)\n",
    "    G1.add_edge(word_id_dict[tweet.user],tweetFormat,weight=1.0)\n",
    "    #G1.add_edge(YEAR_TAG,tweet.date.year,weight=15.0)\n",
    "    #G.add_edge(tweet.date.year,YEAR_TAG,weight=1.0)\n",
    "    for word in tweet.text.split():\n",
    "        G1.add_edge(word_id_dict[word],tweetFormat,weight=1.0)\n",
    "        #print(word)\n",
    "    #monthFormat = \"{month}.{year}\".format(month=tweet.date.month,year=tweet.date.year)\n",
    "\n",
    "    #G1.add_edge(str(tweet.date.year),monthFormat,weight=1.0)\n",
    "    #G1.add_edge(monthFormat,str(tweet.date.year),weight=1.0)\n",
    "\n",
    "    #dayFormat = \"{day}.{month}.{year}\".format(day=tweet.date.day,month=tweet.date.month,year=tweet.date.year)\n",
    "    #G1.add_edge(monthFormat,dayFormat,weight=1.0)\n",
    "    #G1.add_edge(dayFormat,tweetFormat,weight=1.0)\n",
    "    return G1\n",
    "\n",
    "def create_random_graph(tweets, filePath, graph_scale):\n",
    "    \"\"\"\n",
    "\n",
    "    :param type: the graph type\n",
    "    :param filePath: the output file path\n",
    "    :param numberOfCase: the number of examples\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(filePath, \"w+\", encoding='utf-8') as f:\n",
    "        degree = 0.0\n",
    "        for i in range(len(tweets)):\n",
    "            info = {}\n",
    "            graph_node_size = graph_scale\n",
    "            graph_real_node_size = graph_scale\n",
    "            edge_prob = 0.3\n",
    "\n",
    "            edge_count = 0.0\n",
    "\n",
    "            graph = create_graph(tweets[i])\n",
    "            #graph = nx.convert_node_labels_to_integers(graph,first_label=0)\n",
    "            #graph = nx.DiGraph(),\n",
    "            k = len(graph.nodes)\n",
    "            graph_real_node_size = k\n",
    "            ancestor = \"\"\n",
    "            if k < 30:\n",
    "                while k <= 30:\n",
    "                    graph.add_node(str(k))\n",
    "                    k+=1\n",
    "\n",
    "            for id in graph.edges:\n",
    "                edge_count += len(graph.edges[id])\n",
    "            graph_node_size = len(graph.nodes)\n",
    "            start = 0\n",
    "            #nodes = list(graph.nodes)\n",
    "            end = len(graph.nodes)-1\n",
    "            #path = nx.shortest_path(graph, nodes[0], nodes[len(graph.nodes)-1])\n",
    "            #paths = [p for p in nx.all_shortest_paths(graph, nodes[0], nodes[len(graph.nodes)-1])]\n",
    "            #if len(path) >= 3 and len(paths) == 1:\n",
    "            #    degree += edge_count / len(graph.nodes)\n",
    "            #    break\n",
    "\n",
    "\n",
    "            adj_list = []\n",
    "            for adj in graph.adjacency():\n",
    "                adj_list.append(list(adj[1].keys()))\n",
    "            g_nodes = []\n",
    "            for node in graph.nodes:\n",
    "                g_nodes.append(node)\n",
    "            #print(adj_list[0])\n",
    "\n",
    "            g_ids = {}\n",
    "            g_ids_features = {}\n",
    "            g_adj = {}\n",
    "            #print(graph_node_size,start,end)\n",
    "            for j in range(graph_node_size):\n",
    "                g_ids[j] = int(g_nodes[j])\n",
    "                if j == start:\n",
    "                    g_ids_features[j] = \"START\"\n",
    "                elif j == end:\n",
    "                    g_ids_features[j] = \"END\"\n",
    "                else:\n",
    "                    #g_ids_features[i] = str(i+10)\n",
    "                    g_ids_features[j] = str(random.randint(1, end))\n",
    "                g_adj[j] = adj_list[j]\n",
    "\n",
    "            # print start, end, path\n",
    "            text = \"START \"+ tweets[i].user + \" END \"\n",
    "            #for id in range(len(path)):\n",
    "            #    text += g_ids_features[nodes[id]] + \" \"\n",
    "\n",
    "            info[\"seq\"] = text.strip()\n",
    "            info[\"g_ids\"] = g_ids\n",
    "            info['g_ids_features'] = g_ids_features\n",
    "            info['g_adj'] = g_adj\n",
    "            f.write((json.dumps(info,ensure_ascii=False)+\"\\n\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gündem': 12, '2017-01-01': 1, 'Terör': 2, 'saldırısında': 3, '39': 4, 'kişi': 5, 'hayatını': 6, 'kaybetti,': 7, \"4'ü\": 8, 'ağır': 9, '65': 10, 'yaralandı.': 11, \"Ortaköy'de\": 13, 'gerçekleştirilen': 14, 'silahlı': 15, 'saldırı': 16, 'sonrası': 17, 'Cumhurbaşkanı': 18, 'Erdoğan,': 19, 'telefonla': 20, 'bilgi': 21, 'aldı.': 22, 'RTÜK,': 23, \"Ortaköy'deki\": 24, 'terör': 25, 'saldırısı': 26, 'geçici': 27, 'yayın': 28, 'kısıtlamasına': 29, 'gitti.': 30, 'Diyaneş': 31, 'İşleri': 32, 'Başkanı': 33, 'Görmez,': 34, 'saldırının': 35, 'amacının': 36, 'yaşam': 37, 'biçimlerine': 38, 'göre': 39, 'toplumu': 40, 'bölmek': 41, 've': 42, 'karşı': 43, 'karşıya': 44, 'getirmek': 45, 'olduğunu': 46, 'belirterek,': 47, 'Bu...': 48, 'saldırıya': 49, 'dünyadan': 50, 'tepki': 51, 'yağarken;': 52, 'ABD': 53, 'Obama,': 54, 'gerekli': 55, 'destek': 56, 'yardımın': 57, 'yapılması': 58, 'hususunda': 59, 'ekibine': 60, 'talimat': 61, 'verdi': 62, 'açıklama': 63, 'yapan': 64, 'Bakan': 65, 'Bozdağ,': 66, 'Hiçbir': 67, 'birliğimizi': 68, 'bozamayacak,': 69, 'kardeşliğimizi': 70, 'yok': 71, 'edemeyecek...': 72, 'ABD,': 73, \"Türkiye'deki\": 74, 'vatandaşlarının': 75, 'olduğu': 76, 'bölgeden': 77, 'uzak': 78, 'durmalarını': 79, 'istedi.': 80, 'Sağlık': 81, 'Bakanı': 82, 'Akdağ,': 83, 'sonrası,': 84, 'kişinin': 85, 'hastanelerde': 86, 'tedavi': 87, 'gördüğünü,': 88, 'dört': 89, 'durumunun': 90, 'olduğunu...': 91, 'Türk': 92, 'Hava': 93, 'Kuvvetlerine': 94, 'ait': 95, 'uçaklar': 96, 'bölgedeki': 97, 'DEAŞ': 98, 'hedeflerini': 99, 'vurdu;': 100, '34': 101, 'terörist': 102, 'etkisiz': 103, 'hale': 104, 'getirildi,': 105, '17': 106, 'bina': 107, 'imha': 108, 'edildi.': 109, 'Apple': 110, \"CEO'su\": 111, 'Cook,': 112, 'sosyal': 113, 'medya': 114, 'hesabı': 115, 'üzerinden': 116, \"Türkiye'yle\": 117, 'dayanışma': 118, 'içinde': 119, 'ifade': 120, 'eden': 121, 'bir': 122, 'mesaj': 123, 'paylaştı.': 124, 'Bugüne': 125, 'kadar': 126, 'pilot': 127, 'illerde': 128, '260': 129, 'bin': 130, 'başvuruda': 131, 'bulunduğu': 132, 'çipli': 133, 'kimlik': 134, 'kartları,': 135, 'yarından': 136, 'itibaren': 137, 'tüm': 138, 'alınabilecek.': 139, 'İngiltere': 140, 'Büyükelçiliği,': 141, 'vatandaşlarına': 142, 'dikkatli': 143, 'olma': 144, 'uyarısında': 145, 'bulundu.': 146, \"İstanbul'daki\": 147, 'hükümet': 148, 'kanadından': 149, 'yapılan': 150, 'açıklamalarda': 151, 'kınandı': 152, 'mücadelenin': 153, 'süreceği': 154, 'bildirildi.': 155, 'Başbakan...': 156, 'yazılı': 157, 'yaptı...': 158, 'Başbakan': 159, 'Yıldırım': 160, 'başkanlığında': 161, 'ile': 162, 'ilgili': 163, 'toplantı': 164, 'düzenlendi.': 165, 'Rusya': 166, 'Devlet': 167, 'Putin,': 168, 'Bizim': 169, 'ortak': 170, 'görevimiz': 171, 'agresifine': 172, 'kararlı': 173, 'şekilde': 174, 'yanıt': 175, 'vermek': 176, 'mesajını...': 177, 'CHP': 178, 'Lideri': 179, 'Kılıçdaroğlu,': 180, 'saldırısıyla': 181, 'olarak,': 182, 'örgütlerini,': 183, 'toplumsal': 184, 'dayanışmamızla': 185, 'bertaraf': 186, 'edeceğiz': 187, 'dedi': 188, 'atılan...': 189, 'HDP,': 190, 'Bu': 191, 'insanlık': 192, 'düşmanı': 193, 'saldırıyı': 194, 'en': 195, 'sert': 196, 'biçimde': 197, 'kınıyor': 198, 'lanetliyoruz': 199, 'açıklamasını': 200, 'yaptı.': 201, 'TBMM': 202, 'Başkanvekili': 203, 'Hamzaçebi,': 204, 'saldırısının': 205, 'birleştirmesi': 206, 'gerektiğini': 207, 'yası': 208, 'hepimizin': 209, 'tutması': 210, 'gerekir,': 211, 'bu': 212, 'hepimiz...': 213, 'İş': 214, 'dünyası': 215, 'temsilcileri,': 216, 'yayımladıkları': 217, 'mesajlarla': 218, 'terörün': 219, 'insanlığın': 220, 'belirterek': 221, 'lanetledi;': 222, 'devletin': 223, 'verdiği': 224, 'mücadelenin...': 225}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tweets = read_data_from_database()\n",
    "    word_id_dict = create_word_id_dictionary(tweets)\n",
    "    print(word_id_dict)\n",
    "    create_random_graph(tweets, \"./Graph2Seq/data/no_cycle/train.data\", graph_scale=10)\n",
    "    create_random_graph(tweets, \"./Graph2Seq/data/no_cycle/dev.data\", graph_scale=10)\n",
    "    create_random_graph(tweets, \"./Graph2Seq/data/no_cycle/test.data\", graph_scale=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Gündem': 12,\n",
       " '2017-01-01': 1,\n",
       " 'Terör': 2,\n",
       " 'saldırısında': 3,\n",
       " '39': 4,\n",
       " 'kişi': 5,\n",
       " 'hayatını': 6,\n",
       " 'kaybetti,': 7,\n",
       " \"4'ü\": 8,\n",
       " 'ağır': 9,\n",
       " '65': 10,\n",
       " 'yaralandı.': 11,\n",
       " \"Ortaköy'de\": 13,\n",
       " 'gerçekleştirilen': 14,\n",
       " 'silahlı': 15,\n",
       " 'saldırı': 16,\n",
       " 'sonrası': 17,\n",
       " 'Cumhurbaşkanı': 18,\n",
       " 'Erdoğan,': 19,\n",
       " 'telefonla': 20,\n",
       " 'bilgi': 21,\n",
       " 'aldı.': 22,\n",
       " 'RTÜK,': 23,\n",
       " \"Ortaköy'deki\": 24,\n",
       " 'terör': 25,\n",
       " 'saldırısı': 26,\n",
       " 'geçici': 27,\n",
       " 'yayın': 28,\n",
       " 'kısıtlamasına': 29,\n",
       " 'gitti.': 30,\n",
       " 'Diyaneş': 31,\n",
       " 'İşleri': 32,\n",
       " 'Başkanı': 33,\n",
       " 'Görmez,': 34,\n",
       " 'saldırının': 35,\n",
       " 'amacının': 36,\n",
       " 'yaşam': 37,\n",
       " 'biçimlerine': 38,\n",
       " 'göre': 39,\n",
       " 'toplumu': 40,\n",
       " 'bölmek': 41,\n",
       " 've': 42,\n",
       " 'karşı': 43,\n",
       " 'karşıya': 44,\n",
       " 'getirmek': 45,\n",
       " 'olduğunu': 46,\n",
       " 'belirterek,': 47,\n",
       " 'Bu...': 48,\n",
       " 'saldırıya': 49,\n",
       " 'dünyadan': 50,\n",
       " 'tepki': 51,\n",
       " 'yağarken;': 52,\n",
       " 'ABD': 53,\n",
       " 'Obama,': 54,\n",
       " 'gerekli': 55,\n",
       " 'destek': 56,\n",
       " 'yardımın': 57,\n",
       " 'yapılması': 58,\n",
       " 'hususunda': 59,\n",
       " 'ekibine': 60,\n",
       " 'talimat': 61,\n",
       " 'verdi': 62,\n",
       " 'açıklama': 63,\n",
       " 'yapan': 64,\n",
       " 'Bakan': 65,\n",
       " 'Bozdağ,': 66,\n",
       " 'Hiçbir': 67,\n",
       " 'birliğimizi': 68,\n",
       " 'bozamayacak,': 69,\n",
       " 'kardeşliğimizi': 70,\n",
       " 'yok': 71,\n",
       " 'edemeyecek...': 72,\n",
       " 'ABD,': 73,\n",
       " \"Türkiye'deki\": 74,\n",
       " 'vatandaşlarının': 75,\n",
       " 'olduğu': 76,\n",
       " 'bölgeden': 77,\n",
       " 'uzak': 78,\n",
       " 'durmalarını': 79,\n",
       " 'istedi.': 80,\n",
       " 'Sağlık': 81,\n",
       " 'Bakanı': 82,\n",
       " 'Akdağ,': 83,\n",
       " 'sonrası,': 84,\n",
       " 'kişinin': 85,\n",
       " 'hastanelerde': 86,\n",
       " 'tedavi': 87,\n",
       " 'gördüğünü,': 88,\n",
       " 'dört': 89,\n",
       " 'durumunun': 90,\n",
       " 'olduğunu...': 91,\n",
       " 'Türk': 92,\n",
       " 'Hava': 93,\n",
       " 'Kuvvetlerine': 94,\n",
       " 'ait': 95,\n",
       " 'uçaklar': 96,\n",
       " 'bölgedeki': 97,\n",
       " 'DEAŞ': 98,\n",
       " 'hedeflerini': 99,\n",
       " 'vurdu;': 100,\n",
       " '34': 101,\n",
       " 'terörist': 102,\n",
       " 'etkisiz': 103,\n",
       " 'hale': 104,\n",
       " 'getirildi,': 105,\n",
       " '17': 106,\n",
       " 'bina': 107,\n",
       " 'imha': 108,\n",
       " 'edildi.': 109,\n",
       " 'Apple': 110,\n",
       " \"CEO'su\": 111,\n",
       " 'Cook,': 112,\n",
       " 'sosyal': 113,\n",
       " 'medya': 114,\n",
       " 'hesabı': 115,\n",
       " 'üzerinden': 116,\n",
       " \"Türkiye'yle\": 117,\n",
       " 'dayanışma': 118,\n",
       " 'içinde': 119,\n",
       " 'ifade': 120,\n",
       " 'eden': 121,\n",
       " 'bir': 122,\n",
       " 'mesaj': 123,\n",
       " 'paylaştı.': 124,\n",
       " 'Bugüne': 125,\n",
       " 'kadar': 126,\n",
       " 'pilot': 127,\n",
       " 'illerde': 128,\n",
       " '260': 129,\n",
       " 'bin': 130,\n",
       " 'başvuruda': 131,\n",
       " 'bulunduğu': 132,\n",
       " 'çipli': 133,\n",
       " 'kimlik': 134,\n",
       " 'kartları,': 135,\n",
       " 'yarından': 136,\n",
       " 'itibaren': 137,\n",
       " 'tüm': 138,\n",
       " 'alınabilecek.': 139,\n",
       " 'İngiltere': 140,\n",
       " 'Büyükelçiliği,': 141,\n",
       " 'vatandaşlarına': 142,\n",
       " 'dikkatli': 143,\n",
       " 'olma': 144,\n",
       " 'uyarısında': 145,\n",
       " 'bulundu.': 146,\n",
       " \"İstanbul'daki\": 147,\n",
       " 'hükümet': 148,\n",
       " 'kanadından': 149,\n",
       " 'yapılan': 150,\n",
       " 'açıklamalarda': 151,\n",
       " 'kınandı': 152,\n",
       " 'mücadelenin': 153,\n",
       " 'süreceği': 154,\n",
       " 'bildirildi.': 155,\n",
       " 'Başbakan...': 156,\n",
       " 'yazılı': 157,\n",
       " 'yaptı...': 158,\n",
       " 'Başbakan': 159,\n",
       " 'Yıldırım': 160,\n",
       " 'başkanlığında': 161,\n",
       " 'ile': 162,\n",
       " 'ilgili': 163,\n",
       " 'toplantı': 164,\n",
       " 'düzenlendi.': 165,\n",
       " 'Rusya': 166,\n",
       " 'Devlet': 167,\n",
       " 'Putin,': 168,\n",
       " 'Bizim': 169,\n",
       " 'ortak': 170,\n",
       " 'görevimiz': 171,\n",
       " 'agresifine': 172,\n",
       " 'kararlı': 173,\n",
       " 'şekilde': 174,\n",
       " 'yanıt': 175,\n",
       " 'vermek': 176,\n",
       " 'mesajını...': 177,\n",
       " 'CHP': 178,\n",
       " 'Lideri': 179,\n",
       " 'Kılıçdaroğlu,': 180,\n",
       " 'saldırısıyla': 181,\n",
       " 'olarak,': 182,\n",
       " 'örgütlerini,': 183,\n",
       " 'toplumsal': 184,\n",
       " 'dayanışmamızla': 185,\n",
       " 'bertaraf': 186,\n",
       " 'edeceğiz': 187,\n",
       " 'dedi': 188,\n",
       " 'atılan...': 189,\n",
       " 'HDP,': 190,\n",
       " 'Bu': 191,\n",
       " 'insanlık': 192,\n",
       " 'düşmanı': 193,\n",
       " 'saldırıyı': 194,\n",
       " 'en': 195,\n",
       " 'sert': 196,\n",
       " 'biçimde': 197,\n",
       " 'kınıyor': 198,\n",
       " 'lanetliyoruz': 199,\n",
       " 'açıklamasını': 200,\n",
       " 'yaptı.': 201,\n",
       " 'TBMM': 202,\n",
       " 'Başkanvekili': 203,\n",
       " 'Hamzaçebi,': 204,\n",
       " 'saldırısının': 205,\n",
       " 'birleştirmesi': 206,\n",
       " 'gerektiğini': 207,\n",
       " 'yası': 208,\n",
       " 'hepimizin': 209,\n",
       " 'tutması': 210,\n",
       " 'gerekir,': 211,\n",
       " 'bu': 212,\n",
       " 'hepimiz...': 213,\n",
       " 'İş': 214,\n",
       " 'dünyası': 215,\n",
       " 'temsilcileri,': 216,\n",
       " 'yayımladıkları': 217,\n",
       " 'mesajlarla': 218,\n",
       " 'terörün': 219,\n",
       " 'insanlığın': 220,\n",
       " 'belirterek': 221,\n",
       " 'lanetledi;': 222,\n",
       " 'devletin': 223,\n",
       " 'verdiği': 224,\n",
       " 'mücadelenin...': 225}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = read_data_from_database()\n",
    "create_word_id_dictionary(tweets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
