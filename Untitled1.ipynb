{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import Graph2Seq.main.configure as conf\n",
    "import Graph2Seq.main.data_collector as data_collector\n",
    "import Graph2Seq.main.loaderAndwriter as disk_helper\n",
    "import numpy as np\n",
    "from Graph2Seq.main.model import Graph2SeqNN\n",
    "import tensorflow as tf\n",
    "import Graph2Seq.main.helpers as helpers\n",
    "import datetime\n",
    "import Graph2Seq.main.text_decoder as text_decoder\n",
    "from Graph2Seq.main.evaluator import evaluate\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "def main(mode):\n",
    "\n",
    "    word_idx = {}\n",
    "\n",
    "    if mode == \"train\":\n",
    "        epochs = conf.epochs\n",
    "        train_batch_size = conf.train_batch_size\n",
    "\n",
    "        # read the training data from a file\n",
    "        print(\"reading training data into the mem ...\")\n",
    "        texts_train, graphs_train = data_collector.read_data(conf.train_data_path, word_idx, if_increase_dict=True)\n",
    "\n",
    "        print(\"reading development data into the mem ...\")\n",
    "        texts_dev, graphs_dev = data_collector.read_data(conf.dev_data_path, word_idx, if_increase_dict=False)\n",
    "\n",
    "        print(\"writing word-idx mapping ...\")\n",
    "        disk_helper.write_word_idx(word_idx, conf.word_idx_file_path)\n",
    "\n",
    "        print(\"vectoring training data ...\")\n",
    "        tv_train = data_collector.vectorize_data(word_idx, texts_train)\n",
    "\n",
    "        print(\"vectoring dev data ...\")\n",
    "        tv_dev = data_collector.vectorize_data(word_idx, texts_dev)\n",
    "\n",
    "        conf.word_vocab_size = len(word_idx.keys()) + 1\n",
    "\n",
    "        with tf.Graph().as_default():\n",
    "            with tf.Session() as sess:\n",
    "                model = Graph2SeqNN(\"train\", conf, path_embed_method=\"lstm\")\n",
    "\n",
    "                model._build_graph()\n",
    "                saver = tf.train.Saver(max_to_keep=None)\n",
    "                sess.run(tf.initialize_all_variables())\n",
    "\n",
    "                def train_step(seqs, decoder_seq_length, loss_weights, batch_graph, if_pred_on_dev=False):\n",
    "                    dict = {}\n",
    "                    dict['seq'] = seqs\n",
    "                    dict['batch_graph'] = batch_graph\n",
    "                    dict['loss_weights'] = loss_weights\n",
    "                    dict['decoder_seq_length'] = decoder_seq_length\n",
    "\n",
    "                    if not if_pred_on_dev:\n",
    "                        _, loss_op, cross_entropy = model.act(sess, \"train\", dict, if_pred_on_dev)\n",
    "                        return loss_op, cross_entropy\n",
    "                    else:\n",
    "                        sample_id = model.act(sess, \"train\", dict, if_pred_on_dev)\n",
    "                        return sample_id\n",
    "\n",
    "                best_acc_on_dev = 0.0\n",
    "                for t in range(1, epochs + 1):\n",
    "                    n_train = len(texts_train)\n",
    "                    temp_order = list(range(n_train))\n",
    "                    np.random.shuffle(temp_order)\n",
    "\n",
    "                    loss_sum = 0.0\n",
    "                    for start in range(0, n_train, train_batch_size):\n",
    "                        end = min(start+train_batch_size, n_train)\n",
    "                        tv = []\n",
    "                        graphs = []\n",
    "                        for _ in range(start, end):\n",
    "                            idx = temp_order[_]\n",
    "                            tv.append(tv_train[idx])\n",
    "                            graphs.append(graphs_train[idx])\n",
    "\n",
    "                        batch_graph = data_collector.cons_batch_graph(graphs)\n",
    "                        gv = data_collector.vectorize_batch_graph(batch_graph, word_idx)\n",
    "\n",
    "                        tv, tv_real_len, loss_weights = helpers.batch(tv)\n",
    "\n",
    "                        loss_op, cross_entropy = train_step(tv, tv_real_len, loss_weights, gv)\n",
    "                        loss_sum += loss_op\n",
    "\n",
    "                    #################### test the model on the dev data #########################\n",
    "                    n_dev = len(texts_dev)\n",
    "                    dev_batch_size = conf.dev_batch_size\n",
    "\n",
    "                    idx_word = {}\n",
    "                    for w in word_idx:\n",
    "                        idx_word[word_idx[w]] = w\n",
    "\n",
    "                    pred_texts = []\n",
    "                    for start in range(0, n_dev, dev_batch_size):\n",
    "                        end = min(start+dev_batch_size, n_dev)\n",
    "                        tv = []\n",
    "                        graphs = []\n",
    "                        for _ in range(start, end):\n",
    "                            tv.append(tv_dev[_])\n",
    "                            graphs.append(graphs_dev[_])\n",
    "\n",
    "                        batch_graph = data_collector.cons_batch_graph(graphs)\n",
    "                        gv = data_collector.vectorize_batch_graph(batch_graph, word_idx)\n",
    "\n",
    "                        tv, tv_real_len, loss_weights = helpers.batch(tv)\n",
    "\n",
    "                        sample_id = train_step(tv, tv_real_len, loss_weights, gv, if_pred_on_dev=True)[0]\n",
    "\n",
    "                        for tmp_id in sample_id:\n",
    "                            pred_texts.append(text_decoder.decode_text(tmp_id, idx_word))\n",
    "\n",
    "                    acc = evaluate(type=\"acc\", golds=texts_dev, preds=pred_texts)\n",
    "                    if_save_model = False\n",
    "                    if acc >= best_acc_on_dev:\n",
    "                        best_acc_on_dev = acc\n",
    "                        if_save_model = True\n",
    "\n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print('-----------------------')\n",
    "                    print('time:{}'.format(time_str))\n",
    "                    print('Epoch', t)\n",
    "                    print('Acc on Dev: {}'.format(acc))\n",
    "                    print('Best acc on Dev: {}'.format(best_acc_on_dev))\n",
    "                    print('Loss on train:{}'.format(loss_sum))\n",
    "                    if if_save_model:\n",
    "                        save_path = \"./Graph2Seq/saved_model/\"\n",
    "                        if not os.path.exists(save_path):\n",
    "                            os.makedirs(save_path)\n",
    "\n",
    "                        path = saver.save(sess, save_path + 'model', global_step=0)\n",
    "                        print(\"Already saved model to {}\".format(path))\n",
    "\n",
    "                    print('-----------------------')\n",
    "\n",
    "    elif mode == \"test\":\n",
    "        test_batch_size = conf.test_batch_size\n",
    "\n",
    "        # read the test data from a file\n",
    "        print(\"reading test data into the mem ...\")\n",
    "        texts_test, graphs_test = data_collector.read_data(conf.test_data_path, word_idx, if_increase_dict=False)\n",
    "\n",
    "        print(\"reading word idx mapping from file\")\n",
    "        word_idx = disk_helper.read_word_idx_from_file(conf.word_idx_file_path)\n",
    "\n",
    "        idx_word = {}\n",
    "        for w in word_idx:\n",
    "            idx_word[word_idx[w]] = w\n",
    "\n",
    "        print(\"vectoring test data ...\")\n",
    "        tv_test = data_collector.vectorize_data(word_idx, texts_test)\n",
    "\n",
    "        conf.word_vocab_size = len(word_idx.keys()) + 1\n",
    "\n",
    "        with tf.Graph().as_default():\n",
    "            with tf.Session() as sess:\n",
    "                model = Graph2SeqNN(\"test\", conf, path_embed_method=\"lstm\")\n",
    "                model._build_graph()\n",
    "                saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "                model_path_name = \"./Graph2Seq/saved_model/model-0\"\n",
    "                model_pred_path = \"./Graph2Seq/saved_model/prediction.txt\"\n",
    "\n",
    "                saver.restore(sess, model_path_name)\n",
    "\n",
    "                def test_step(seqs, decoder_seq_length, loss_weights, batch_graph):\n",
    "                    dict = {}\n",
    "                    dict['seq'] = seqs\n",
    "                    dict['batch_graph'] = batch_graph\n",
    "                    dict['loss_weights'] = loss_weights\n",
    "                    dict['decoder_seq_length'] = decoder_seq_length\n",
    "                    sample_id = model.act(sess, \"test\", dict, if_pred_on_dev=False)\n",
    "                    return sample_id\n",
    "\n",
    "                n_test = len(texts_test)\n",
    "\n",
    "                pred_texts = []\n",
    "                global_graphs = []\n",
    "                for start in range(0, n_test, test_batch_size):\n",
    "                    end = min(start + test_batch_size, n_test)\n",
    "                    tv = []\n",
    "                    graphs = []\n",
    "                    for _ in range(start, end):\n",
    "                        tv.append(tv_test[_])\n",
    "                        graphs.append(graphs_test[_])\n",
    "                        global_graphs.append(graphs_test[_])\n",
    "\n",
    "                    batch_graph = data_collector.cons_batch_graph(graphs)\n",
    "                    gv = data_collector.vectorize_batch_graph(batch_graph, word_idx)\n",
    "                    tv, tv_real_len, loss_weights = helpers.batch(tv)\n",
    "\n",
    "                    sample_id = test_step(tv, tv_real_len, loss_weights, gv)[0]\n",
    "                    for tem_id in sample_id:\n",
    "                        pred_texts.append(text_decoder.decode_text(tem_id, idx_word))\n",
    "\n",
    "                acc = evaluate(type=\"acc\", golds=texts_test, preds=pred_texts)\n",
    "                print(\"acc on test set is {}\".format(acc))\n",
    "\n",
    "                # write prediction result into a file\n",
    "                with open(model_pred_path, 'w+') as f:\n",
    "                    for _ in range(len(global_graphs)):\n",
    "                        f.write(\"graph:\\t\"+json.dumps(global_graphs[_])+\"\\nGold:\\t\"+texts_test[_]+\"\\nPredicted:\\t\"+pred_texts[_]+\"\\n\")\n",
    "                        if texts_test[_].strip() ==  pred_texts[_].strip():\n",
    "                            f.write(\"Correct\\n\\n\")\n",
    "                        else:\n",
    "                            f.write(\"Incorrect\\n\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading training data into the mem ...\n",
      "reading development data into the mem ...\n",
      "writing word-idx mapping ...\n",
      "vectoring training data ...\n",
      "vectoring dev data ...\n",
      "WARNING:tensorflow:From C:\\Users\\Helin\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Helin\\Desktop\\bitirme\\graph\\Graph2Seq\\main\\aggregators.py:112: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Helin\\Desktop\\bitirme\\graph\\Graph2Seq\\main\\model.py:277: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Users\\Helin\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Helin\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Helin\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Helin\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "-----------------------\n",
      "time:2019-05-24T02:36:32.840901\n",
      "Epoch 1\n",
      "Acc on Dev: 1.0\n",
      "Best acc on Dev: 1.0\n",
      "Loss on train:14.453627586364746\n",
      "INFO:tensorflow:./Graph2Seq/saved_model/model-0 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Already saved model to ./Graph2Seq/saved_model/model-0\n",
      "-----------------------\n",
      "-----------------------\n",
      "time:2019-05-24T02:36:33.755098\n",
      "Epoch 2\n",
      "Acc on Dev: 1.0\n",
      "Best acc on Dev: 1.0\n",
      "Loss on train:14.006617546081543\n",
      "INFO:tensorflow:./Graph2Seq/saved_model/model-0 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Already saved model to ./Graph2Seq/saved_model/model-0\n",
      "-----------------------\n",
      "-----------------------\n",
      "time:2019-05-24T02:36:34.538102\n",
      "Epoch 3\n",
      "Acc on Dev: 1.0\n",
      "Best acc on Dev: 1.0\n",
      "Loss on train:13.495859146118164\n",
      "INFO:tensorflow:./Graph2Seq/saved_model/model-0 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Already saved model to ./Graph2Seq/saved_model/model-0\n",
      "-----------------------\n",
      "-----------------------\n",
      "time:2019-05-24T02:36:35.323113\n",
      "Epoch 4\n",
      "Acc on Dev: 1.0\n",
      "Best acc on Dev: 1.0\n",
      "Loss on train:12.794443130493164\n",
      "INFO:tensorflow:./Graph2Seq/saved_model/model-0 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Already saved model to ./Graph2Seq/saved_model/model-0\n",
      "-----------------------\n",
      "-----------------------\n",
      "time:2019-05-24T02:36:36.142822\n",
      "Epoch 5\n",
      "Acc on Dev: 0.0\n",
      "Best acc on Dev: 1.0\n",
      "Loss on train:11.758122444152832\n",
      "-----------------------\n",
      "-----------------------\n",
      "time:2019-05-24T02:36:36.306722\n",
      "Epoch 6\n",
      "Acc on Dev: 0.0\n",
      "Best acc on Dev: 1.0\n",
      "Loss on train:10.15396499633789\n",
      "-----------------------\n",
      "-----------------------\n",
      "time:2019-05-24T02:36:36.506598\n",
      "Epoch 7\n",
      "Acc on Dev: 0.0\n",
      "Best acc on Dev: 1.0\n",
      "Loss on train:7.87230920791626\n",
      "-----------------------\n",
      "-----------------------\n",
      "time:2019-05-24T02:36:36.624527\n",
      "Epoch 8\n",
      "Acc on Dev: 0.0\n",
      "Best acc on Dev: 1.0\n",
      "Loss on train:5.549371242523193\n",
      "-----------------------\n",
      "-----------------------\n",
      "time:2019-05-24T02:36:36.740454\n",
      "Epoch 9\n",
      "Acc on Dev: 0.0\n",
      "Best acc on Dev: 1.0\n",
      "Loss on train:4.111661911010742\n",
      "-----------------------\n",
      "-----------------------\n",
      "time:2019-05-24T02:36:36.862379\n",
      "Epoch 10\n",
      "Acc on Dev: 1.0\n",
      "Best acc on Dev: 1.0\n",
      "Loss on train:3.162167549133301\n",
      "INFO:tensorflow:./Graph2Seq/saved_model/model-0 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Already saved model to ./Graph2Seq/saved_model/model-0\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #argparser = argparse.ArgumentParser()\n",
    "    #argparser.add_argument(\"-mode\", type=str, choices=[\"train\", \"test\"])\n",
    "    #argparser.add_argument(\"-sample_size_per_layer\", type=int, default=4, help=\"sample size at each layer\")\n",
    "    #argparser.add_argument(\"-sample_layer_size\", type=int, default=4, help=\"sample layer size\")\n",
    "    #argparser.add_argument(\"-epochs\", type=int, default=100, help=\"training epochs\")\n",
    "    #argparser.add_argument(\"-learning_rate\", type=float, default=conf.learning_rate, help=\"learning rate\")\n",
    "    #argparser.add_argument(\"-word_embedding_dim\", type=int, default=conf.word_embedding_dim, help=\"word embedding dim\")\n",
    "    #argparser.add_argument(\"-hidden_layer_dim\", type=int, default=conf.hidden_layer_dim)\n",
    "\n",
    "    #config = argparser.parse_args()\n",
    "\n",
    "    mode = \"train\"\n",
    "    conf.sample_layer_size = 4\n",
    "    conf.sample_size_per_layer = 4\n",
    "    conf.epochs = 10\n",
    "    conf.learning_rate = 0.001\n",
    "    conf.word_embedding_dim = 100\n",
    "    conf.hidden_layer_dim = 100\n",
    "\n",
    "    main(mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading test data into the mem ...\n",
      "reading word idx mapping from file\n",
      "vectoring test data ...\n",
      "WARNING:tensorflow:From C:\\Users\\Helin\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Helin\\Desktop\\bitirme\\graph\\Graph2Seq\\main\\model.py:277: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Users\\Helin\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./Graph2Seq/saved_model/model-0\n",
      "acc on test set is 1.0\n"
     ]
    }
   ],
   "source": [
    "mode = \"test\"\n",
    "conf.sample_layer_size = 4\n",
    "conf.sample_size_per_layer = 4\n",
    "conf.epochs = 10\n",
    "conf.learning_rate = 0.001\n",
    "conf.word_embedding_dim = 100\n",
    "conf.hidden_layer_dim = 100\n",
    "\n",
    "main(mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
